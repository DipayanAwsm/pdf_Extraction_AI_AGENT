#!/usr/bin/env python3
"""
Standalone PDF to Claude extractor using AWS Bedrock.
Extracts text from PDFs and sends to Claude for table extraction.
Exports results to both JSON and Excel formats.
"""

import boto3
import json
import argparse
from pathlib import Path
from typing import List, Dict, Any
import pandas as pd
import re

from src.claim_extractor.extract_text import extract_text_from_pdf


EVALUATION_DATE_PATTERNS = [
    r"\b(?:evaluation\s*date|as\s*of|report\s*date|run\s*date|valuation\s*date)\s*[:\-]?\s*([A-Za-z]{3,9}\s+\d{1,2},\s*\d{4}|\d{1,2}[/-]\d{1,2}[/-]\d{2,4}|\d{4}[/-]\d{1,2}[/-]\d{1,2})",
]

CARRIER_PATTERNS = [
    r"\b(?:carrier|company|insurer|provider)\s*[:\-]\s*([A-Za-z0-9 &'.\-/]+)",
]


def setup_aws_client(access_key: str, secret_key: str, session_token: str, region: str):
    """Setup AWS Bedrock client with provided credentials."""
    try:
        session = boto3.Session(
            aws_access_key_id=access_key,
            aws_secret_access_key=secret_key,
            aws_session_token=session_token,
            region_name=region
        )
        bedrock = session.client('bedrock-runtime')
        return bedrock
    except Exception as e:
        print(f"‚ùå Failed to setup AWS client: {str(e)}")
        return None


def extract_text_from_pdf_page_by_page(pdf_path: str, use_ocr_fallback: bool = True, dpi: int = 300):
    """Extract text from PDF page by page to handle large documents better."""
    import pdfplumber
    from pdf2image import convert_from_path
    import pytesseract
    
    all_text = []
    used_ocr = False
    
    try:
        # Try native text extraction first
        with pdfplumber.open(pdf_path) as pdf:
            total_pages = len(pdf.pages)
            print(f"üìÑ Total pages in PDF: {total_pages}")
            
            for page_num, page in enumerate(pdf.pages, 1):
                print(f"üìñ Processing page {page_num}/{total_pages}...")
                
                # Extract text from current page
                page_text = page.extract_text(x_tolerance=2, y_tolerance=2) or ""
                
                if not page_text.strip() and use_ocr_fallback:
                    # If no text, try OCR for this page
                    print(f"üì∑ Using OCR for page {page_num}...")
                    images = convert_from_path(pdf_path, dpi=dpi, first_page=page_num, last_page=page_num)
                    if images:
                        page_text = pytesseract.image_to_string(images[0], lang="eng")
                        used_ocr = True
                
                if page_text.strip():
                    all_text.append(f"--- PAGE {page_num} ---\n{page_text.strip()}")
                    print(f"‚úÖ Page {page_num}: {len(page_text)} characters")
                else:
                    print(f"‚ö†Ô∏è Page {page_num}: No text extracted")
                    all_text.append(f"--- PAGE {page_num} ---\n[No text content]")
                
                # Add a small delay to prevent overwhelming the system
                import time
                time.sleep(0.1)
        
        return "\n\n".join(all_text), used_ocr
        
    except Exception as e:
        print(f"‚ùå Error in page-by-page extraction: {str(e)}")
        # Fallback to original method
        return extract_text_from_pdf(pdf_path, use_ocr_fallback, dpi)


def detect_line_of_business(pdf_text: str) -> str:
    """Detect the line of business from PDF content."""
    pdf_text_upper = pdf_text.upper()
    
    # Define business type patterns
    business_patterns = {
        'AUTO': [
            'AUTO', 'AUTOMOBILE', 'VEHICLE', 'CAR', 'TRUCK', 'MOTOR',
            'AUTO LIABILITY', 'AUTO PHYSICAL DAMAGE', 'PERSONAL AUTO',
            'COMMERCIAL AUTO', 'GARAGE LIABILITY', 'MOTOR CARRIER'
        ],
        'GENERAL LIABILITY': [
            'GENERAL LIABILITY', 'GL', 'COMMERCIAL GENERAL LIABILITY',
            'CGL', 'PROPERTY', 'LIABILITY', 'BUSINESS LIABILITY',
            'PROFESSIONAL LIABILITY', 'PRODUCTS LIABILITY'
        ],
        'WC': [
            'WORKERS COMPENSATION', 'WORKER COMPENSATION', 'WC',
            'WORKERS COMP', 'EMPLOYER LIABILITY', 'WORK COMP',
            'WORKERS COMPENSATION AND EMPLOYERS LIABILITY'
        ]
    }
    
    # Count matches for each business type
    business_scores = {}
    for business_type, patterns in business_patterns.items():
        score = 0
        for pattern in patterns:
            if pattern in pdf_text_upper:
                score += pdf_text_upper.count(pattern)
        business_scores[business_type] = score
    
    # Find the business type with highest score
    if business_scores:
        detected_business = max(business_scores, key=business_scores.get)
        if business_scores[detected_business] > 0:
            print(f"üè¢ Detected Line of Business: {detected_business} (Score: {business_scores[detected_business]})")
            return detected_business
    
    print("‚ö†Ô∏è Could not detect specific line of business")
    return "UNKNOWN"


def load_aws_config_from_py(config_file: str = "config.py") -> Dict[str, str]:
    """Load AWS credentials and model configuration from Python config file."""
    config_path = Path(config_file)
    
    if not config_path.exists():
        print(f"‚ùå Configuration file not found: {config_path}")
        print("Please create config.py with your AWS credentials and model:")
        print("""
# AWS Configuration
AWS_ACCESS_KEY = "YOUR_ACCESS_KEY_ID"
AWS_SECRET_KEY = "YOUR_SECRET_ACCESS_KEY"
AWS_SESSION_TOKEN = "YOUR_SESSION_TOKEN"
AWS_REGION = "us-east-1"
MODEL_ID = "anthropic.claude-3-sonnet-20240229-v1:0"
        """)
        return None
    
    try:
        # Import the config file as a module
        import importlib.util
        spec = importlib.util.spec_from_file_location("config", config_path)
        config_module = importlib.util.module_from_spec(spec)
        spec.loader.exec_module(config_module)
        
        # Extract configuration values
        config = {
            'access_key': getattr(config_module, 'AWS_ACCESS_KEY', None),
            'secret_key': getattr(config_module, 'AWS_SECRET_KEY', None),
            'session_token': getattr(config_module, 'AWS_SESSION_TOKEN', None),
            'region': getattr(config_module, 'AWS_REGION', None),
            'model_id': getattr(config_module, 'MODEL_ID', None),
            'max_chunk_size': getattr(config_module, 'MAX_CHUNK_SIZE', 15000),
            'api_delay': getattr(config_module, 'API_DELAY', 1)
        }
        
        # Validate required fields
        required_fields = ['access_key', 'secret_key', 'session_token', 'region', 'model_id']
        missing_fields = [field for field in required_fields if not config[field]]
        
        if missing_fields:
            print(f"‚ùå Missing required fields in config: {missing_fields}")
            return None
        
        # Validate model ID format
        valid_model_prefixes = [
            "anthropic.claude-3-sonnet",
            "anthropic.claude-3-haiku", 
            "anthropic.claude-3-opus",
            "anthropic.claude-v2",
            "anthropic.claude-instant"
        ]
        
        model_is_valid = any(config['model_id'].startswith(prefix) for prefix in valid_model_prefixes)
        if not model_is_valid:
            print(f"‚ö†Ô∏è Warning: Model ID '{config['model_id']}' may not be valid")
            print("Valid model prefixes:", valid_model_prefixes)
        
        print(f"‚úÖ Configuration loaded from: {config_path}")
        print(f"ü§ñ Model configured: {config['model_id']}")
        print(f"üåç AWS Region: {config['region']}")
        
        return config
        
    except Exception as e:
        print(f"‚ùå Error reading config file: {e}")
        return None


def extract_tables_with_claude_page_by_page(bedrock_client, pdf_text: str, pdf_name: str, model_id: str, max_chunk_size: int = 15000, api_delay: int = 1) -> List[Dict[str, Any]]:
    """Extract tables from PDF text using AWS Bedrock Claude with better handling of large content."""
    
    # Split text into manageable chunks if it's very long
    text_chunks = []
    
    if len(pdf_text) > max_chunk_size:
        print(f"üìè Text is very long ({len(pdf_text)} chars), splitting into chunks...")
        # Split by page boundaries
        pages = pdf_text.split("--- PAGE")
        current_chunk = ""
        
        for page in pages:
            if not page.strip():
                continue
            if len(current_chunk) + len(page) > max_chunk_size and current_chunk:
                text_chunks.append(current_chunk.strip())
                current_chunk = page
            else:
                current_chunk += "\n--- PAGE" + page
        
        if current_chunk.strip():
            text_chunks.append(current_chunk.strip())
        
        print(f"üì¶ Split into {len(text_chunks)} chunks")
    else:
        text_chunks = [pdf_text]
    
    all_tables = []
    
    for chunk_idx, chunk in enumerate(text_chunks, 1):
        print(f"ü§ñ Processing chunk {chunk_idx}/{len(text_chunks)} ({len(chunk)} chars)...")
        
        prompt = f"""
        You are an expert at extracting structured data from insurance and claims documents. 
        
        Please analyze the following PDF content chunk and extract ALL tables and structured data you can find.
        This is chunk {chunk_idx} of {len(text_chunks)} from the document.
        
        IMPORTANT: Also identify the Line of Business from these options:
        1. AUTO - Automobile insurance, vehicle claims, motor vehicle liability
        2. GENERAL LIABILITY - Commercial general liability, property, business liability
        3. WC - Workers Compensation, employer liability, work comp claims
        
        For each table or structured section, provide:
        1. A descriptive name for the table/section
        2. The extracted data in a structured format (JSON)
        3. Any relevant metadata (headers, row counts, etc.)
        4. Line of Business classification
        
        PDF Name: {pdf_name}
        Chunk: {chunk_idx}/{len(text_chunks)}
        
        Content:
        {chunk}
        
        Please respond with a JSON array where each element represents a table/section:
        [
            {{
                "table_name": "Description of the table",
                "headers": ["Column1", "Column2", "Column3"],
                "data": [
                    ["Row1Col1", "Row1Col2", "Row1Col3"],
                    ["Row2Col1", "Row2Col2", "Row2Col3"]
                ],
                "metadata": {{
                    "row_count": 2,
                    "column_count": 3,
                    "description": "Brief description of what this table contains",
                    "chunk_source": {chunk_idx},
                    "line_of_business": "AUTO|GENERAL LIABILITY|WC"
                }}
            }}
        ]
        
        If no structured tables are found, extract any organized information in a table-like format.
        Focus on finding tables, lists, and structured data in this chunk.
        Be sure to classify the Line of Business based on the content.
        """
        
        try:
            response = bedrock_client.invoke_model(
                modelId=model_id,
                body=json.dumps({
                    "anthropic_version": "bedrock-2023-05-31",
                    "max_tokens": 4000,
                    "messages": [
                        {
                            "role": "user",
                            "content": prompt
                        }
                    ]
                })
            )
            
            response_body = json.loads(response['body'].read())
            content = response_body['content'][0]['text']
            
            print(f"‚úÖ Claude response received for chunk {chunk_idx}")
            
            # Try to extract JSON from Claude's response
            try:
                start_idx = content.find('[')
                end_idx = content.rfind(']') + 1
                if start_idx != -1 and end_idx != -1:
                    json_content = content[start_idx:end_idx]
                    chunk_tables = json.loads(json_content)
                    print(f"üìä Extracted {len(chunk_tables)} tables from chunk {chunk_idx}")
                    all_tables.extend(chunk_tables)
                else:
                    print(f"‚ö†Ô∏è Could not parse structured response from Claude for chunk {chunk_idx}")
            except json.JSONDecodeError as e:
                print(f"‚ö†Ô∏è Could not parse JSON response from Claude for chunk {chunk_idx}: {e}")
                
        except Exception as e:
            print(f"‚ùå Error calling AWS Bedrock for chunk {chunk_idx}: {str(e)}")
            continue
        
        # Add delay between chunks to prevent rate limiting
        import time
        time.sleep(api_delay)
    
    print(f"üéØ Total tables extracted across all chunks: {len(all_tables)}")
    return all_tables


def save_to_excel(tables_data: List[Dict[str, Any]], output_path: str, pdf_name: str = None):
    """Save extracted tables to Excel file with different sheets."""
    try:
        with pd.ExcelWriter(output_path, engine='openpyxl') as writer:
            # Create a summary sheet
            summary_data = []
            for i, table in enumerate(tables_data):
                summary_data.append({
                    'Sheet_Name': f"Table_{i+1}",
                    'Table_Name': table.get('table_name', f'Table {i+1}'),
                    'Source_File': table.get('source_file', pdf_name or 'Unknown'),
                    'Extraction_Method': table.get('extraction_method', 'Unknown'),
                    'Row_Count': len(table.get('data', [])),
                    'Column_Count': len(table.get('headers', [])),
                    'Description': table.get('metadata', {}).get('description', 'N/A')
                })
            
            summary_df = pd.DataFrame(summary_data)
            summary_df.to_excel(writer, sheet_name='Summary', index=False)
            
            # Create individual table sheets
            for i, table in enumerate(tables_data):
                if not table.get('data') or not table.get('headers'):
                    continue
                    
                # Create DataFrame
                df = pd.DataFrame(table['data'], columns=table['headers'])
                
                # Clean sheet name (Excel has restrictions)
                sheet_name = f"Table_{i+1}_{table.get('table_name', 'Unknown')[:20]}"
                sheet_name = "".join(c for c in sheet_name if c.isalnum() or c in (' ', '_', '-'))[:31]
                
                # Write to Excel
                df.to_excel(writer, sheet_name=sheet_name, index=False)
                
                # Add metadata sheet for each table
                if table.get('metadata'):
                    metadata_df = pd.DataFrame([table['metadata']])
                    metadata_sheet_name = f"Meta_{i+1}"[:31]
                    metadata_df.to_excel(writer, sheet_name=metadata_sheet_name, index=False)
        
        print(f"üíæ Excel file saved: {output_path}")
        return True
        
    except Exception as e:
        print(f"‚ùå Error saving Excel file: {str(e)}")
        return False


def _lob_folder_name(line_of_business: str) -> str:
    lob = (line_of_business or "UNKNOWN").upper().strip()
    if lob == "AUTO":
        return "auto"
    if lob in ("GENERAL LIABILITY", "GL"):
        return "GL"
    if lob == "WC":
        return "WC"
    return "UNKNOWN"


def classify_line_of_business_via_bedrock(bedrock_client, pdf_text: str, model_id: str) -> str:
    """Classify line of business (AUTO, GENERAL LIABILITY, WC) using Claude via Bedrock.

    Returns one of: 'AUTO', 'GENERAL LIABILITY', 'WC', or 'UNKNOWN' on failure.
    """
    try:
        prompt = f"""
You are an insurance domain expert. Read the following document text and classify the dominant line of business.
Choose exactly one from this set:
- AUTO
- GENERAL LIABILITY
- WC

Return ONLY strict JSON: {{"line_of_business": "AUTO|GENERAL LIABILITY|WC"}} with no extra text.

Document text:
{pdf_text}
"""
        response = bedrock_client.invoke_model(
            modelId=model_id,
            body=json.dumps({
                "anthropic_version": "bedrock-2023-05-31",
                "max_tokens": 100,
                "temperature": 0.0,
                "messages": [
                    {"role": "user", "content": prompt}
                ],
            }),
        )
        response_body = json.loads(response["body"].read())
        content = response_body["content"][0]["text"]
        # Extract JSON only
        start_idx = content.find("{")
        end_idx = content.rfind("}") + 1
        if start_idx != -1 and end_idx > start_idx:
            try:
                obj = json.loads(content[start_idx:end_idx])
                lob = (obj.get("line_of_business") or "").strip().upper()
                if lob in {"AUTO", "GENERAL LIABILITY", "WC"}:
                    return lob
            except Exception:
                pass
        print("‚ö†Ô∏è Claude LOB classification not parseable; falling back to heuristic.")
        return "UNKNOWN"
    except Exception as exc:
        print(f"‚ùå LOB classification via Bedrock failed: {exc}")
        return "UNKNOWN"


def extract_lob_fields_via_bedrock(bedrock_client, pdf_text: str, model_id: str, lob: str) -> dict:
    """Use LLM to extract normalized fields per LOB directly from full text.

    Returns a dict with keys:
      - evaluation_date: str (YYYY-MM-DD if possible)
      - carrier: str
      - claims: List[Dict[...] per LOB schema]
    or empty dict on failure.
    """
    try:
        lob = (lob or "").upper()
        if lob == 'AUTO':
            schema = {
                "evaluation_date": "string",
                "carrier": "string",
                "claims": [
                    {
                        "claim_number": "string",
                        "loss_date": "string",
                        "paid_loss": "string",
                        "reserve": "string",
                        "alae": "string"
                    }
                ]
            }
            guidance = "For AUTO, extract: evaluation_date, carrier, and per-claim fields: claim_number, loss_date, paid_loss, reserve, alae."
        elif lob in ('GENERAL LIABILITY','GL'):
            schema = {
                "evaluation_date": "string",
                "carrier": "string",
                "claims": [
                    {
                        "claim_number": "string",
                        "loss_date": "string",
                        "bi_paid_loss": "string",
                        "pd_paid_loss": "string",
                        "bi_reserve": "string",
                        "pd_reserve": "string",
                        "alae": "string"
                    }
                ]
            }
            guidance = "For GL, extract: evaluation_date, carrier, and per-claim: bi_paid_loss, pd_paid_loss, bi_reserve, pd_reserve, alae."
        elif lob == 'WC':
            schema = {
                "evaluation_date": "string",
                "carrier": "string",
                "claims": [
                    {
                        "claim_number": "string",
                        "loss_date": "string",
                        "bi_paid_loss": "string",
                        "pd_paid_loss": "string",
                        "bi_reserve": "string",
                        "pd_reserve": "string",
                        "alae": "string"
                    }
                ]
            }
            guidance = "For WC, extract: evaluation_date, carrier, and per-claim: bi_paid_loss, pd_paid_loss, bi_reserve, pd_reserve, alae."
        else:
            return {}

        prompt = f"""
You are an insurance data extraction assistant.
Given the document text, extract the following fields for line of business: {lob}.
{guidance}

Return STRICT JSON ONLY (no prose) matching this schema:
{schema}

Normalization rules:
- Return dates in ISO format if possible (YYYY-MM-DD). If not sure, keep as found.
- Keep currency/amount strings as-is (do not calculate).
- If a field is missing, use an empty string.
- Claims should be in document order.

Document text:
{pdf_text}
"""
        response = bedrock_client.invoke_model(
            modelId=model_id,
            body=json.dumps({
                "anthropic_version": "bedrock-2023-05-31",
                "max_tokens": 4000,
                "temperature": 0.0,
                "messages": [{"role":"user","content": prompt}]
            })
        )
        response_body = json.loads(response['body'].read())
        content = response_body['content'][0]['text']
        start = content.find('{')
        end = content.rfind('}') + 1
        if start != -1 and end > start:
            obj = json.loads(content[start:end])
            # Basic shape check
            if isinstance(obj, dict) and 'claims' in obj:
                # Ensure keys exist
                obj.setdefault('evaluation_date', '')
                obj.setdefault('carrier', '')
                if not isinstance(obj.get('claims'), list):
                    obj['claims'] = []
                return obj
        return {}
    except Exception as e:
        print(f"‚ö†Ô∏è LLM structured extraction failed: {e}")
        return {}


def process_pdf(pdf_path: str, bedrock_client, output_dir: str = None, model_id: str = None, max_chunk_size: int = 15000, api_delay: int = 1):
    """Process a single PDF file and extract tables using Claude."""
    pdf_path = Path(pdf_path)
    
    if not pdf_path.exists():
        print(f"‚ùå PDF file not found: {pdf_path}")
        return None
    
    print(f"\nüìÑ Processing: {pdf_path.name}")
    print("=" * 50)
    
    try:
        # Extract complete text from PDF page by page (no truncation)
        print("üìñ Extracting complete text from PDF page by page...")
        pdf_text, used_ocr = extract_text_from_pdf_page_by_page(str(pdf_path), use_ocr_fallback=True)
        
        if used_ocr:
            print(f"üì∑ OCR used for some pages (scanned content)")
        else:
            print(f"üìù Text extraction used for all pages")
        
        print(f"üìè Complete text length: {len(pdf_text)} characters")
        print(f"üìÑ Text preview (first 200 chars): {pdf_text[:200]}...")
        
        if not pdf_text.strip():
            print("‚ö†Ô∏è No text extracted from PDF. File may be image-only or corrupted.")
            return []
        
        # Detect line of business
        line_of_business = classify_line_of_business_via_bedrock(bedrock_client, pdf_text, model_id)
        if line_of_business == "UNKNOWN":
            line_of_business = detect_line_of_business(pdf_text)
        
        # Extract tables using Claude with complete text (page by page approach)
        print("ü§ñ Sending complete text to Claude for table extraction...")
        tables = extract_tables_with_claude_page_by_page(bedrock_client, pdf_text, pdf_path.name, model_id, max_chunk_size, api_delay)
        
        if tables:
            # Optionally augment with Camelot tables
            try:
                from config import USE_CAMELOT, CAMELOT_FLAVORS, CAMELOT_PAGES
            except Exception:
                USE_CAMELOT, CAMELOT_FLAVORS, CAMELOT_PAGES = False, [], "all"
            if USE_CAMELOT:
                camelot_tables = _try_camelot_tables(str(pdf_path), CAMELOT_FLAVORS or ["lattice","stream"], CAMELOT_PAGES or "all")
                if camelot_tables:
                    tables.extend(camelot_tables)
                    print(f"‚ûï Augmented with {len(camelot_tables)} Camelot tables")

            # Add file information to tables
            for table in tables:
                table['source_file'] = pdf_path.name
                table['extraction_method'] = 'OCR' if used_ocr else 'Text'
                table['total_text_length'] = len(pdf_text)
                table['line_of_business'] = line_of_business
                
                # Ensure metadata has line of business
                if 'metadata' not in table:
                    table['metadata'] = {}
                table['metadata']['line_of_business'] = line_of_business
            
            # Create filename with line of business
            safe_business_name = line_of_business.replace(' ', '_').replace('&', 'AND')
            base_filename = f"{pdf_path.stem}_{safe_business_name}"
            
            # Determine subfolder by LOB and ensure it exists
            lob_folder = _lob_folder_name(line_of_business)
            target_dir = Path(output_dir) / lob_folder if output_dir else Path.cwd() / lob_folder
            target_dir.mkdir(parents=True, exist_ok=True)
            
            # Save results to JSON
            if output_dir:
                json_path = target_dir / f"{base_filename}_claude_results.json"
                with open(json_path, 'w', encoding='utf-8') as f:
                    json.dump(tables, f, indent=2, ensure_ascii=False)
                print(f"üíæ JSON results saved to: {json_path}")
                
                # Save to Excel (raw extracted tables)
                excel_path = target_dir / f"{base_filename}_claude_results.xlsx"
                save_to_excel(tables, str(excel_path), pdf_path.name)

                # Normalized per LOB using LLM first, fallback to heuristic table normalization
                normalized = None
                llm_norm = extract_lob_fields_via_bedrock(bedrock_client, pdf_text, model_id, line_of_business)

                if line_of_business == 'AUTO':
                    normalized = llm_norm if llm_norm else normalize_auto_records(tables, pdf_text)
                    norm_excel_path = target_dir / f"{base_filename}_AUTO_normalized.xlsx"
                    save_auto_normalized_excel(normalized, str(norm_excel_path))
                    print(f"üíæ AUTO normalized Excel saved to: {norm_excel_path}")
                elif line_of_business in ('GENERAL LIABILITY', 'GL'):
                    normalized = llm_norm if llm_norm else normalize_gl_records(tables, pdf_text)
                    norm_excel_path = target_dir / f"{base_filename}_GL_normalized.xlsx"
                    save_gl_normalized_excel(normalized, str(norm_excel_path))
                    print(f"üíæ GL normalized Excel saved to: {norm_excel_path}")
                elif line_of_business == 'WC':
                    normalized = llm_norm if llm_norm else normalize_wc_records(tables, pdf_text)
                    norm_excel_path = target_dir / f"{base_filename}_WC_normalized.xlsx"
                    save_wc_normalized_excel(normalized, str(norm_excel_path))
                    print(f"üíæ WC normalized Excel saved to: {norm_excel_path}")
            else:
                normalized = None
            
            # Display summary
            print(f"üìä Summary for {pdf_path.name}:")
            print(f"üè¢ Line of Business: {line_of_business}")
            for i, table in enumerate(tables):
                table_name = table.get('table_name', f'Table {i+1}')
                headers = table.get('headers', [])
                data = table.get('data', [])
                chunk_source = table.get('metadata', {}).get('chunk_source', 'Unknown')
                print(f"  Table {i+1}: {table_name}")
                print(f"    Headers: {headers}")
                print(f"    Rows: {len(data)}")
                print(f"    Columns: {len(headers)}")
                print(f"    Source: Chunk {chunk_source}")
            
            return {
                'lob': line_of_business,
                'normalized': normalized,
                'source_file': pdf_path.name,
            }
        else:
            print(f"‚ö†Ô∏è No tables found in {pdf_path.name}")
            return {'lob': line_of_business, 'normalized': None, 'source_file': pdf_path.name}
            
    except Exception as e:
        print(f"‚ùå Error processing {pdf_path.name}: {str(e)}")
        return None


def _write_consolidated_excels(collected: dict, output_dir: Path):
    import pandas as pd
    # collected keys: 'AUTO', 'GL', 'WC' ‚Üí list of normalized dicts (also may include 'GENERAL LIABILITY')
    auto_df = None
    gl_df = None
    wc_df = None

    # AUTO
    entries = []
    for entry in (collected.get('AUTO') or []):
        eval_date = entry.get('evaluation_date', '')
        carrier = entry.get('carrier', '')
        for claim in entry.get('claims', []):
            row = {
                'evaluation_date': eval_date,
                'carrier': claim.get('carrier') or carrier,
                'claim_number': claim.get('claim_number', ''),
                'loss_date': claim.get('loss_date', ''),
                'paid_loss': claim.get('paid_loss', ''),
                'reserve': claim.get('reserve', ''),
                'alae': claim.get('alae', ''),
            }
            entries.append(row)
    if entries:
        auto_df = pd.DataFrame(entries, columns=['evaluation_date','carrier','claim_number','loss_date','paid_loss','reserve','alae'])
        out_dir = output_dir / 'auto'
        out_dir.mkdir(parents=True, exist_ok=True)
        out_path = out_dir / 'AUTO_consolidated.xlsx'
        with pd.ExcelWriter(out_path, engine='openpyxl') as writer:
            auto_df.to_excel(writer, sheet_name='auto_claims', index=False)
        print(f"üìö Consolidated AUTO Excel written: {out_path}")

    # GL (merge keys 'GL' and 'GENERAL LIABILITY')
    entries = []
    for entry in (collected.get('GL') or []) + (collected.get('GENERAL LIABILITY') or []):
        eval_date = entry.get('evaluation_date', '')
        carrier = entry.get('carrier', '')
        for claim in entry.get('claims', []):
            row = {
                'evaluation_date': eval_date,
                'carrier': claim.get('carrier') or carrier,
                'claim_number': claim.get('claim_number', ''),
                'loss_date': claim.get('loss_date', ''),
                'bi_paid_loss': claim.get('bi_paid_loss', ''),
                'pd_paid_loss': claim.get('pd_paid_loss', ''),
                'bi_reserve': claim.get('bi_reserve', ''),
                'pd_reserve': claim.get('pd_reserve', ''),
                'alae': claim.get('alae', ''),
            }
            entries.append(row)
    if entries:
        gl_df = pd.DataFrame(entries, columns=['evaluation_date','carrier','claim_number','loss_date','bi_paid_loss','pd_paid_loss','bi_reserve','pd_reserve','alae'])
        out_dir = output_dir / 'GL'
        out_dir.mkdir(parents=True, exist_ok=True)
        out_path = out_dir / 'GL_consolidated.xlsx'
        with pd.ExcelWriter(out_path, engine='openpyxl') as writer:
            gl_df.to_excel(writer, sheet_name='gl_claims', index=False)
        print(f"üìö Consolidated GL Excel written: {out_path}")

    # WC
    entries = []
    for entry in (collected.get('WC') or []):
        eval_date = entry.get('evaluation_date', '')
        carrier = entry.get('carrier', '')
        for claim in entry.get('claims', []):
            row = {
                'evaluation_date': eval_date,
                'carrier': claim.get('carrier') or carrier,
                'claim_number': claim.get('claim_number', ''),
                'loss_date': claim.get('loss_date', ''),
                'bi_paid_loss': claim.get('bi_paid_loss', ''),
                'pd_paid_loss': claim.get('pd_paid_loss', ''),
                'bi_reserve': claim.get('bi_reserve', ''),
                'pd_reserve': claim.get('pd_reserve', ''),
                'alae': claim.get('alae', ''),
            }
            entries.append(row)
    if entries:
        wc_df = pd.DataFrame(entries, columns=['evaluation_date','carrier','claim_number','loss_date','bi_paid_loss','pd_paid_loss','bi_reserve','pd_reserve','alae'])
        out_dir = output_dir / 'WC'
        out_dir.mkdir(parents=True, exist_ok=True)
        out_path = out_dir / 'WC_consolidated.xlsx'
        with pd.ExcelWriter(out_path, engine='openpyxl') as writer:
            wc_df.to_excel(writer, sheet_name='wc_claims', index=False)
        print(f"üìö Consolidated WC Excel written: {out_path}")

    # Combined result.xlsx at root with available sheets
    combined_path = output_dir / 'result.xlsx'
    if any([auto_df is not None, gl_df is not None, wc_df is not None]):
        with pd.ExcelWriter(combined_path, engine='openpyxl') as writer:
            if auto_df is not None:
                auto_df.to_excel(writer, sheet_name='auto_claims', index=False)
            if gl_df is not None:
                gl_df.to_excel(writer, sheet_name='gl_claims', index=False)
            if wc_df is not None:
                wc_df.to_excel(writer, sheet_name='wc_claims', index=False)
        print(f"üìò Combined consolidated Excel written: {combined_path}")


def _first_group(patterns, text: str) -> str:
    for pat in patterns:
        m = re.search(pat, text, flags=re.IGNORECASE)
        if m:
            return m.group(1).strip()
    return ""


def _normalize_date_str(s: str) -> str:
    try:
        from dateutil import parser as date_parser
        return date_parser.parse(s, fuzzy=True).strftime("%Y-%m-%d")
    except Exception:
        return s or ""


def _find_col_idx(headers, *candidates):
    if not headers:
        return None
    lower = [str(h).strip().lower() for h in headers]
    for cand in candidates:
        cand_l = cand.lower()
        if cand_l in lower:
            return lower.index(cand_l)
    # fuzzy contains
    for i, h in enumerate(lower):
        for cand in candidates:
            if cand.lower() in h:
                return i
    return None


def normalize_auto_records(tables: list, pdf_text: str) -> dict:
    eval_raw = _first_group(EVALUATION_DATE_PATTERNS, pdf_text)
    evaluation_date = _normalize_date_str(eval_raw) if eval_raw else ""
    carrier = _first_group(CARRIER_PATTERNS, pdf_text)

    claims = []
    for t in tables:
        headers = t.get('headers') or []
        rows = t.get('data') or []
        if not headers or not rows:
            continue
        i_claim = _find_col_idx(headers, 'claim number', 'claim no', 'claim#', 'reference', 'ref')
        i_loss_date = _find_col_idx(headers, 'loss date', 'date of loss', 'dol', 'accident date')
        i_paid = _find_col_idx(headers, 'paid loss', 'paid', 'indemnity paid', 'total paid')
        i_reserve = _find_col_idx(headers, 'reserve', 'reserves', 'loss reserve', 'remaining reserve')
        i_alae = _find_col_idx(headers, 'alae', 'allocated loss adjustment expense', 'expense', 'total expense')
        i_carrier = _find_col_idx(headers, 'carrier', 'company', 'insurer', 'provider')

        for r in rows:
            rec = {
                'carrier': carrier or (str(r[i_carrier]).strip() if i_carrier is not None and i_carrier < len(r) else ''),
                'claim_number': (str(r[i_claim]).strip() if i_claim is not None and i_claim < len(r) else ''),
                'loss_date': _normalize_date_str(str(r[i_loss_date]).strip()) if i_loss_date is not None and i_loss_date < len(r) else '',
                'paid_loss': str(r[i_paid]).strip() if i_paid is not None and i_paid < len(r) else '',
                'reserve': str(r[i_reserve]).strip() if i_reserve is not None and i_reserve < len(r) else '',
                'alae': str(r[i_alae]).strip() if i_alae is not None and i_alae < len(r) else '',
            }
            # skip empty rows
            if any(rec.values()):
                claims.append(rec)

    return {
        'evaluation_date': evaluation_date,
        'carrier': carrier,
        'claims': claims,
    }


def save_auto_normalized_excel(norm: dict, excel_path: str):
    import pandas as pd
    with pd.ExcelWriter(excel_path, engine='openpyxl') as writer:
        # meta
        meta_df = pd.DataFrame([{
            'evaluation_date': norm.get('evaluation_date', ''),
            'carrier': norm.get('carrier', ''),
        }])
        meta_df.to_excel(writer, sheet_name='meta', index=False)
        # claims
        claims = norm.get('claims') or []
        if claims:
            claims_df = pd.DataFrame(claims, columns=['carrier','claim_number','loss_date','paid_loss','reserve','alae'])
            claims_df.to_excel(writer, sheet_name='auto_claims', index=False)
        else:
            pd.DataFrame(columns=['carrier','claim_number','loss_date','paid_loss','reserve','alae']).to_excel(writer, sheet_name='auto_claims', index=False)


# GL and WC normalization

def normalize_gl_records(tables: list, pdf_text: str) -> dict:
    eval_raw = _first_group(EVALUATION_DATE_PATTERNS, pdf_text)
    evaluation_date = _normalize_date_str(eval_raw) if eval_raw else ""
    carrier = _first_group(CARRIER_PATTERNS, pdf_text)

    claims = []
    for t in tables:
        headers = t.get('headers') or []
        rows = t.get('data') or []
        if not headers or not rows:
            continue
        i_claim = _find_col_idx(headers, 'claim number', 'claim no', 'claim#', 'reference', 'ref')
        i_loss_date = _find_col_idx(headers, 'loss date', 'date of loss', 'dol', 'accident date')
        i_bi_paid = _find_col_idx(headers, 'bodily injury paid loss', 'bi paid', 'paid bodily injury')
        i_pd_paid = _find_col_idx(headers, 'property damage paid loss', 'pd paid', 'paid property damage')
        i_bi_res = _find_col_idx(headers, 'bodily injury reserves', 'bi reserve', 'bodily injury reserve')
        i_pd_res = _find_col_idx(headers, 'property damage reserves', 'pd reserve', 'property damage reserve')
        i_alae = _find_col_idx(headers, 'alae', 'allocated loss adjustment expense', 'expense', 'total expense')
        i_carrier = _find_col_idx(headers, 'carrier', 'company', 'insurer', 'provider')

        for r in rows:
            rec = {
                'carrier': carrier or (str(r[i_carrier]).strip() if i_carrier is not None and i_carrier < len(r) else ''),
                'claim_number': (str(r[i_claim]).strip() if i_claim is not None and i_claim < len(r) else ''),
                'loss_date': _normalize_date_str(str(r[i_loss_date]).strip()) if i_loss_date is not None and i_loss_date < len(r) else '',
                'bi_paid_loss': str(r[i_bi_paid]).strip() if i_bi_paid is not None and i_bi_paid < len(r) else '',
                'pd_paid_loss': str(r[i_pd_paid]).strip() if i_pd_paid is not None and i_pd_paid < len(r) else '',
                'bi_reserve': str(r[i_bi_res]).strip() if i_bi_res is not None and i_bi_res < len(r) else '',
                'pd_reserve': str(r[i_pd_res]).strip() if i_pd_res is not None and i_pd_res < len(r) else '',
                'alae': str(r[i_alae]).strip() if i_alae is not None and i_alae < len(r) else '',
            }
            if any(rec.values()):
                claims.append(rec)

    return {
        'evaluation_date': evaluation_date,
        'carrier': carrier,
        'claims': claims,
    }


def save_gl_normalized_excel(norm: dict, excel_path: str):
    import pandas as pd
    with pd.ExcelWriter(excel_path, engine='openpyxl') as writer:
        meta_df = pd.DataFrame([{
            'evaluation_date': norm.get('evaluation_date', ''),
            'carrier': norm.get('carrier', ''),
        }])
        meta_df.to_excel(writer, sheet_name='meta', index=False)
        claims = norm.get('claims') or []
        cols = ['carrier','claim_number','loss_date','bi_paid_loss','pd_paid_loss','bi_reserve','pd_reserve','alae']
        if claims:
            pd.DataFrame(claims, columns=cols).to_excel(writer, sheet_name='gl_claims', index=False)
        else:
            pd.DataFrame(columns=cols).to_excel(writer, sheet_name='gl_claims', index=False)


def normalize_wc_records(tables: list, pdf_text: str) -> dict:
    # As specified, WC uses the same set as GL in this request
    return normalize_gl_records(tables, pdf_text)


def save_wc_normalized_excel(norm: dict, excel_path: str):
    import pandas as pd
    with pd.ExcelWriter(excel_path, engine='openpyxl') as writer:
        meta_df = pd.DataFrame([{
            'evaluation_date': norm.get('evaluation_date', ''),
            'carrier': norm.get('carrier', ''),
        }])
        meta_df.to_excel(writer, sheet_name='meta', index=False)
        claims = norm.get('claims') or []
        cols = ['carrier','claim_number','loss_date','bi_paid_loss','pd_paid_loss','bi_reserve','pd_reserve','alae']
        if claims:
            pd.DataFrame(claims, columns=cols).to_excel(writer, sheet_name='wc_claims', index=False)
        else:
            pd.DataFrame(columns=cols).to_excel(writer, sheet_name='wc_claims', index=False)


def main():
    parser = argparse.ArgumentParser(description="Extract tables from PDFs using Claude via AWS Bedrock")
    parser.add_argument("pdf_path", help="Path to PDF file or directory")
    parser.add_argument("--config", default="config.py", help="Path to Python configuration file (default: config.py)")
    parser.add_argument("--output-dir", default="claude_results", help="Output directory for results (default: claude_results)")
    
    args = parser.parse_args()
    
    # Load AWS configuration
    print("üîë Loading AWS configuration...")
    aws_config = load_aws_config_from_py(args.config)
    
    if not aws_config:
        print("‚ùå Failed to load AWS configuration. Exiting.")
        return
    
    # Create output directory if it doesn't exist
    output_dir = Path(args.output_dir)
    output_dir.mkdir(exist_ok=True)
    print(f"üìÅ Output directory: {output_dir.absolute()}")
    
    # Setup AWS client
    print("üîë Setting up AWS Bedrock client...")
    bedrock_client = setup_aws_client(
        aws_config['access_key'], 
        aws_config['secret_key'], 
        aws_config['session_token'], 
        aws_config['region']
    )
    
    if not bedrock_client:
        print("‚ùå Failed to setup AWS client. Exiting.")
        return
    
    print("‚úÖ AWS Bedrock client ready!")
    
    # Process PDF(s)
    pdf_path = Path(args.pdf_path)
    
    if pdf_path.is_file() and pdf_path.suffix.lower() == '.pdf':
        # Single PDF file
        result = process_pdf(str(pdf_path), bedrock_client, args.output_dir, aws_config['model_id'], 
                   aws_config['max_chunk_size'], aws_config['api_delay'])
        
        if result:
            print(f"\nüìä Summary for {pdf_path.name}:")
            print(f"üè¢ Line of Business: {result['lob']}")
            if result['normalized']:
                print(f"üíæ Normalized data saved to: {output_dir.absolute()}")
                _write_consolidated_excels({result['lob']: [result['normalized']]}, output_dir)
            else:
                print("‚ö†Ô∏è No normalized data available for this PDF.")
            
    elif pdf_path.is_dir():
        # Directory of PDFs
        pdf_files = list(pdf_path.glob("*.pdf"))
        if not pdf_files:
            print(f"‚ùå No PDF files found in directory: {pdf_path}")
            return
        
        print(f"üìÅ Found {len(pdf_files)} PDF files")
        
        collected_by_lob = {'AUTO': [], 'GENERAL LIABILITY': [], 'GL': [], 'WC': []}
        for pdf_file in pdf_files:
            result = process_pdf(str(pdf_file), bedrock_client, args.output_dir, aws_config['model_id'],
                               aws_config['max_chunk_size'], aws_config['api_delay'])
            if result and result.get('normalized'):
                lob = result.get('lob')
                if lob in collected_by_lob:
                    collected_by_lob[lob].append(result['normalized'])
                elif lob == 'GL':
                    collected_by_lob['GL'].append(result['normalized'])
                else:
                    # map unknown to nothing
                    pass
        
        # Write consolidated files
        _write_consolidated_excels(collected_by_lob, output_dir)
        print(f"üìÅ All results saved to: {output_dir.absolute()}")


if __name__ == "__main__":
    main()
